{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\RCP\\ClimbAssistant\\.venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import cv2\n",
    "from shapely.geometry import Polygon\n",
    "import itertools\n",
    "import shutil\n",
    "import os\n",
    "from detectron2.structures import BoxMode\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy files into train and val folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do a train-test split of 80-20 for this iteration\n",
    "train_num = 79 # use 80 photos for training (out of 100)\n",
    "count = 0\n",
    "\n",
    "train_path = \"C:/Users/user/Desktop/boulder photos/iter2/train\"\n",
    "val_path = \"C:/Users/user/Desktop/boulder photos/iter2/val\"\n",
    "\n",
    "bk_path = \"C:/Users/user/Desktop/boulder photos/labelling/bk\"\n",
    "xr_path = \"C:/Users/user/Desktop/boulder photos/labelling/xinrui\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    }
   ],
   "source": [
    "# for bk images\n",
    "for file in os.listdir(bk_path):\n",
    "    if file.endswith('.jpeg'):\n",
    "        file_path = os.path.join(bk_path, file)\n",
    "        shutil.copy(file_path, train_path)\n",
    "        count +=1 \n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# for xr images, some go to validation set\n",
    "for file in os.listdir(xr_path):\n",
    "    if file.endswith('.jpeg'):\n",
    "        file_path = os.path.join(xr_path, file)\n",
    "        if count <= train_num:\n",
    "            shutil.copy(file_path, train_path)\n",
    "        else:\n",
    "            shutil.copy(file_path, val_path)\n",
    "        count +=1 \n",
    "    \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract json data and transform into required COCO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we get the image size\n",
    "# since all taken in iPhone, we just need to check 1 image\n",
    "# im = cv2.imread(train_path + \"/boulderingwallstockimages0.jpeg\")\n",
    "# h, w, c = im.shape\n",
    "# print(h, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we are using only holds for our training, we set the category ID all to the same number\n",
    "cat_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from the bk labels file\n",
    "with open(os.path.join(bk_path, \"labels.json\")) as f:\n",
    "    d = json.load(f)\n",
    "\n",
    "# read data from the xr labels file\n",
    "with open(os.path.join(xr_path, \"labels.json\")) as f:\n",
    "    d1 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open train and mask json\n",
    "with open(os.path.join(train_path, \"annotations.json\")) as f:\n",
    "    train = json.load(f)\n",
    "    \n",
    "with open(os.path.join(val_path, \"annotations.json\")) as f:\n",
    "    val = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR TRAIN SET\n",
    "\n",
    "# we start image & mask ID count from 0\n",
    "train_image_count = 0\n",
    "train_mask_count = 0\n",
    "\n",
    "# store the images we filtered into train dataset\n",
    "train_imgs = []\n",
    "\n",
    "for key in d:\n",
    "        # get the file name and read the dimensions of each image\n",
    "        file_name = d[key]['filename']\n",
    "\n",
    "        file_path = os.path.join(train_path, file_name)\n",
    "\n",
    "        im = cv2.imread(file_path)\n",
    "        h, w, c = im.shape\n",
    "\n",
    "        # this is the dictionary to append to images list\n",
    "        dict_1 = {\n",
    "                \"file_name\": d[key]['filename'],\n",
    "                \"height\": h,\n",
    "                \"width\": w,\n",
    "                \"id\": train_image_count \n",
    "                }\n",
    "\n",
    "        # add into train set\n",
    "        train['images'].append(dict_1)\n",
    "        train_imgs.append(d[key]['filename'])\n",
    "\n",
    "        result = im.copy()\n",
    "                \n",
    "        for segment in d[key]['regions']:\n",
    "                x = segment['shape_attributes']['all_points_x']\n",
    "                y = segment['shape_attributes']['all_points_y']\n",
    "                \n",
    "                # get the area of the polygon from the points\n",
    "                pgon = Polygon(zip(x, y))\n",
    "                area = pgon.area\n",
    "                \n",
    "                # combine the x and y coordinates into 1 alternating list\n",
    "                coords = [i for i in itertools.chain.from_iterable(itertools.zip_longest(x,y)) if i]\n",
    "                \n",
    "                # get bbox coordinates\n",
    "                bbox = [min(x), min(y), max(x) - min(x), max(y) - min(y)] # bbox coordinates must be in the XYWH format!!\n",
    "\n",
    "                # cv2.rectangle(result, (min(x), min(y)), (max(x), max(y)), (0,0,255), 2)\n",
    "                \n",
    "                # this is the dictionary to append to annotations list\n",
    "                dict_2 = {\n",
    "                        \"segmentation\": [ coords ],\n",
    "                        \"area\": area,\n",
    "                        \"iscrowd\": 0,\n",
    "                        \"image_id\": train_image_count,\n",
    "                        \"bbox\": bbox,\n",
    "                        \"bbox_mode\": 0,\n",
    "                        \"category_id\": cat_id,\n",
    "                        \"id\": train_mask_count\n",
    "                        }\n",
    "                \n",
    "                # this logic is used to split real image dataset equally into train n val\n",
    "                train['annotations'].append(dict_2)\n",
    "                \n",
    "                train_mask_count += 1\n",
    "        # plt.imshow(result)\n",
    "        # plt.show()\n",
    "        train_image_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR VALIDATION SET\n",
    "# for xinrui dataset, we need to split into both train n val sets\n",
    "\n",
    "# we start image & mask ID count from 0\n",
    "val_image_count = 0\n",
    "val_mask_count = 0\n",
    "\n",
    "# store the images we filtered into val dataset\n",
    "val_imgs = []\n",
    "\n",
    "# check if the file in train path to add to train annotations, else add to val one\n",
    "train_dir_imgs = os.listdir(train_path)\n",
    "\n",
    "for key in d1:\n",
    "        # get the file name and read the dimensions of each image\n",
    "        file_name = d1[key]['filename']\n",
    "\n",
    "        # if the file has been added to the training directory, we will add it to the train annotations\n",
    "        # else, it will be in the validation annotations\n",
    "        if file_name in train_dir_imgs:\n",
    "                file_path = os.path.join(train_path, file_name)\n",
    "                im = cv2.imread(file_path)\n",
    "                h, w, c = im.shape\n",
    "\n",
    "                # this is the dictionary to append to images list\n",
    "                dict_1 = {\n",
    "                        \"file_name\": d1[key]['filename'],\n",
    "                        \"height\": h,\n",
    "                        \"width\": w,\n",
    "                        \"id\": train_image_count \n",
    "                        }\n",
    "\n",
    "                # add into train or val set\n",
    "                train['images'].append(dict_1)\n",
    "                train_imgs.append(d1[key]['filename'])\n",
    "\n",
    "                        \n",
    "                for segment in d1[key]['regions']:\n",
    "                        x = segment['shape_attributes']['all_points_x']\n",
    "                        y = segment['shape_attributes']['all_points_y']\n",
    "                        \n",
    "                        # get the area of the polygon from the points\n",
    "                        pgon = Polygon(zip(x, y))\n",
    "                        area = pgon.area\n",
    "                        \n",
    "                        # combine the x and y coordinates into 1 alternating list\n",
    "                        coords = [i for i in itertools.chain.from_iterable(itertools.zip_longest(x,y)) if i]\n",
    "                        \n",
    "                        # get bbox coordinates\n",
    "                        bbox = [min(x), min(y), max(x) - min(x), max(y) - min(y)]\n",
    "                        \n",
    "                        # this is the dictionary to append to annotations list\n",
    "                        dict_2 = {\n",
    "                                \"segmentation\": [ coords ],\n",
    "                                \"area\": area,\n",
    "                                \"iscrowd\": 0,\n",
    "                                \"image_id\": train_image_count,\n",
    "                                \"bbox\": bbox,\n",
    "                                \"bbox_mode\": 0,\n",
    "                                \"category_id\": cat_id,\n",
    "                                \"id\": train_mask_count\n",
    "                                }\n",
    "                        \n",
    "                        # this logic is used to split real image dataset equally into train n val\n",
    "                        train['annotations'].append(dict_2)\n",
    "                        \n",
    "                        train_mask_count += 1\n",
    "                        \n",
    "                train_image_count += 1\n",
    "        \n",
    "        else:\n",
    "                file_path = os.path.join(val_path, file_name)\n",
    "                im = cv2.imread(file_path)\n",
    "                h, w, c = im.shape\n",
    "\n",
    "                # this is the dictionary to append to images list\n",
    "                dict_1 = {\n",
    "                        \"file_name\": d1[key]['filename'],\n",
    "                        \"height\": h,\n",
    "                        \"width\": w,\n",
    "                        \"id\": val_image_count \n",
    "                        }\n",
    "\n",
    "                # add into train or val set\n",
    "                val['images'].append(dict_1)\n",
    "                val_imgs.append(d1[key]['filename'])\n",
    "\n",
    "                for segment in d1[key]['regions']:\n",
    "                        x = segment['shape_attributes']['all_points_x']\n",
    "                        y = segment['shape_attributes']['all_points_y']\n",
    "                        \n",
    "                        # get the area of the polygon from the points\n",
    "                        pgon = Polygon(zip(x, y))\n",
    "                        area = pgon.area\n",
    "                        \n",
    "                        # combine the x and y coordinates into 1 alternating list\n",
    "                        coords = [i for i in itertools.chain.from_iterable(itertools.zip_longest(x,y)) if i]\n",
    "                        \n",
    "                        # get bbox coordinates\n",
    "                        bbox = [min(x), min(y), max(x) - min(x), max(y) - min(y)]\n",
    "                        \n",
    "                        # this is the dictionary to append to annotations list\n",
    "                        dict_2 = {\n",
    "                                \"segmentation\": [ coords ],\n",
    "                                \"area\": area,\n",
    "                                \"iscrowd\": 0,\n",
    "                                \"image_id\": val_image_count,\n",
    "                                \"bbox\": bbox,\n",
    "                                \"bbox_mode\": 0,\n",
    "                                \"category_id\": cat_id,\n",
    "                                \"id\": val_mask_count\n",
    "                                }\n",
    "                        \n",
    "                        # this logic is used to split real image dataset equally into train n val\n",
    "                        val['annotations'].append(dict_2)\n",
    "                        \n",
    "                        val_mask_count += 1\n",
    "                        \n",
    "                val_image_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = {\n",
    "            \"supercategory\": \"hold\",\n",
    "            \"id\": 0,\n",
    "            \"name\": \"hold\"\n",
    "        }\n",
    "train['categories'].append(category)\n",
    "val['categories'].append(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new info back to train and mask json\n",
    "with open(os.path.join(train_path, 'annotations.json'), 'w') as f:\n",
    "    json.dump(train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new info back to val and mask json\n",
    "with open(os.path.join(val_path, 'annotations.json'), 'w') as f:\n",
    "    json.dump(val, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "db022c7c5e43c0aaf838eb761152d2d60003010a0d44d8542e2da617ee6a3a8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
